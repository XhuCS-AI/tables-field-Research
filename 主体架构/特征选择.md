补：
点乘：两个向量的对应元素相乘后求和
叉乘: 两个向量按矩阵乘法的规则来
逐元素乘法: 两个向量的对应元素相乘

#### 标准注意力机制

核心目的是：由于本身的嵌入向量的表达并不准确，因此我们想要的是产生一组新的更为精准的嵌入向量，即包含了上下文的信息的新向量。

设原始嵌入向量为 $x\in \mathbb{R}^{b \times d}$ 其中 $b$ 表示批次大小, $d$ 表示嵌入维度。则: $$Q=x\cdot W_q,K=x\cdot W_k,V=x\cdot W_v$$

<<<<<<< HEAD
其中 $Q\in \mathbb{R}^{b \times d_k}$，$K\in \mathbb{R}^{b \times d_k}$，$V\in \mathbb{R}^{b \times d_v}$。公式表达为:
$$Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}})V$$
=======
其中 $Q\in \mathbb{R}^{b \times d_k}$，$K\in \mathbb{R}^{b \times d_k}$，$V\in \mathbb{R}^{b \times d_v}$。公式表达为:$$Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}})V$$
>>>>>>> 3290558aa7cc5298650360ebac4b1afea2225e48

##### 收益来源如下

1. $Q$ 的意义是查询，查询与当前向量有关的关系。具体为使用矩阵 $W_q$ 将高维向量映射到低维查询空间中的某个向量，用向量编码<寻找 xxx >的概念。
2. $K$ 的意义是回答，回答 $Q$ 的问题。同样为使用矩阵 $W_k$ 将高维向量映射到低维回答空间中的某个向量，用向量编码某个问题的答案。
3. $V$ 的作用是修改，可以想象成与 $K$ 相关联的向量。可以理解为，如果目标向量需要修改，那么我应该加上什么向量？

> 标准自注意力中，低维回答空间与低维的查询空间维度是一致的，即 $Q$ 和 $K$ 维度相同。$V$ 的维度也与 $Q/K$ 保持一致或保持输入维度。

为了衡量每个键与每个查询的匹配程度，将 Q 与 K 进行点乘（为了数值稳定，会让每一个数值除以 K-Q 空间维度的平方根）。得到的值可以是负无穷到正无穷的任何实数，值越大表示越相关反之则不相关。同时将他们将经过按行方向的 Softmax 变成概率权重与 V 相乘，最终得到新的嵌入向量。

#### 表格中的注意力理解

相对于标准注意力机制的更正：

- $Q$ 表示一组新的样本的特征
- $K$ 表示我已知的全部样本的特征
- $V$ 表示我全部样本对应的 target

那么用 $\alpha(Q,K)$ 表示 $Q$ 与 $K$ 对应的权重，其中的 $\alpha$ 为任意的可以刻画相关性的函数。$$f(Q)=\alpha(Q,K)V$$
